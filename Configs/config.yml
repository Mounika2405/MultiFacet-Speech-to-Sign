data:
    nonreg_trg: "text"    # Target - text transcription
    trg: "norm_skels.h5"    # Target - 3D body co-ordinates (skels)
    src: "gst.h5"    ## Source - audio features
    text_src: "text_embedding.h5"  # Text embeddings (BERT)
    text_src_mask: "text_embedding_mask.h5"  # Text embeddings mask
    files: "files"    # Filenames for each sequence
    facs: "smoothed_facs_embedding.h5"  # FACS embeddings

    train: "train"  # tar data files for training  
    dev: "dev"  # tar data files for validating
    test: "test" # tar data files for testing
    raw_data_dir: None
    skip_frames: 1   # Skip frames in the data, to reduce the data input size
    nonreg_trg_voc_min_freq: 1
    src_fps: 100 # Source FPS
    trg_fps: 25  # Target FPS
    num_sec: 6 # max audio length in input
    max_sent_length: 300
    num_keypoints: 85 # All keypoints

training:
    random_seed: 27   # Random seed for initialisation
    optimizer: "adam"   # Chosen optimiser (adam, ..)
    learning_rate: 0.001   # Initial model learning rate
    learning_rate_min: 0.0002 # Learning rate minimum, when training will stop
    weight_decay: 0.0   # Weight Decay
    clip_grad_norm: 5.0   # Gradient clipping value
    batch_size: 32 # Batch Size for training
    scheduling: "plateau"   # Scheduling at training time (plateau, ...)
    patience: 100  # How many epochs of no improvement causes a LR reduction
    decrease_factor: 0.7  # LR reduction factor, after the # of patience epochs
    early_stopping_metric: "dtw" # Which metric determines scheduling (DTW, loss, BT...)
    epochs: 100000  # How many epochs to run for
    validation_freq: 5000  # After how many steps to run a validation on the model
    logging_freq: 250  # After how many steps to log training progress
    eval_metric: "dtw"  # Evaluation metric during training (dtw','bt')
    model_dir: "multifacet_model/" # Where the model shall be stored
    overwrite: False # Flag to overwrite a previous saved model in the model_dir
    continue: True  # Flag to continue from a previous saved model in the model_dir
    shuffle: True  # Flag to shuffle the data during training
    use_cuda: True  # Flag to use GPU cuda capabilities
    max_output_length: 300 # Max Output Length
    max_feature_index: 143 # Range to use for dtw (inclusive), Set to 143 (48*3-1) for only body+hand
    save_val_keypoints_dir: "multifacet_model/val_preds"
    keep_last_ckpts: 1 # How many previous best/latest checkpoints to keep
    loss: "L1"  # Loss function (MSE, L1)
    nonreg_loss: "mock" # "mock", anything else will map to XentLoss
    facs_loss: "BCE" # "mock", anything else will map to XentLoss
    regloss_weight: 1
    xentloss_weight: 0.0  # Weight for Cross entropy loss
    advloss_weight: 0.0001  # Weight for adversarial loss 
    bceloss_weight: 0.001  # Weight for Binary Cross entropy logit loss
    disc:
        optimizer: "adam"   # Chosen optimiser (adam, ..)
        learning_rate: 0.001  # Initial model learning rate
        learning_rate_min: 0.0002 # Learning rate minimum, when training will stop
        weight_decay: 0.0

model:
    initializer: "xavier" # Model initialisation (Xavier, ...)
    bias_initializer: "zeros"  # Bias initialiser (Zeros, ...)
    embed_initializer: "xavier" # Embedding initialiser (Xavier, ...)
    src_size: 256 # size of audio features, 80 for melspectrogram
    text_src_size: 768 # size of text embeddings
    trg_size: 255  # Size of target skeleton coordinates (150 for Inverse Kinematics body/hands)
    just_count_in: False # Flag for Just Counter Data Augmentation
    gaussian_noise: False # Flag for Gaussian Noise Data Augmentation
    noise_rate: 1 # Gaussian Noise rate
    future_prediction: 10 # Future Prediction Data Augmentation if > 0
    encoder:  # Model Encoder
        type: "transformer"
        num_layers: 2 # Number of layers
        num_heads: 8  # Number of Heads
        embeddings:
            embedding_dim: 512  # Embedding Dimension
            dropout: 0.0 # Embedding Dropout
        hidden_size: 512 # Hidden Size Dimension
        ff_size: 1024 # Feed-forward dimension (4 x hidden_size)
        dropout: 0.0 # Encoder Dropout
    text_encoder:  # Text Encoder
        type: "transformer"
        num_layers: 2 # Number of layers
        num_heads: 8  # Number of Heads
        embeddings:
            embedding_dim: 512  # Embedding Dimension
            dropout: 0.0 # Embedding Dropout
        hidden_size: 512 # Hidden Size Dimension
        ff_size: 1024 # Feed-forward dimension (4 x hidden_size)
        dropout: 0.0 # Encoder Dropout
    decoder: # Model Decoder
        type: "JointTransformerDecoder"
        num_layers: 2 # Number of layers
        num_heads: 8 # Number of Heads
        embeddings:
            embedding_dim: 512 # Embedding Dimension
            dropout: 0.0 # Embedding Dropout
        hidden_size: 512 # Hidden Size Dimension
        ff_size: 1024 # Feed-forward dimension (4 x hidden_size)
        dropout: 0.0 # Decoder Dropout
    facs_decoder: # FACS Decoder
        type: "transformer"
        trg_size: 41
        num_layers: 2 # Number of layers
        num_heads: 8 # Number of Heads
        embeddings:
            embedding_dim: 512 # Embedding Dimension
            dropout: 0.0 # Embedding Dropout
        hidden_size: 512 # Hidden Size Dimension
        ff_size: 1024 # Feed-forward dimension (4 x hidden_size)
        dropout: 0.0 # Decoder Dropout
    nonreg_decoder:
        type: "mock"